{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fNWTS6vJUkf"
      },
      "source": [
        "# Deployment of an **LLM** & Information Extraction via a **RAG** Approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EIVuSCMrnnD"
      },
      "source": [
        "## **Abstract**\n",
        "This document serves as a guide to explain the concept of Retrieval Augmented Generation LLM (RAG-LLM). The RAG-LLM is a deep learning language model designed to generate text by utilizing both external database information and a deep understanding of natural language. This guide provides an overview of the RAG-LLM's functioning, potential applications, and the steps necessary to use it effectively in various contexts. By offering detailed explanation, this document aims to facilitate the understanding and utilization of the RAG-LLM for those who wish to leverage its enhanced text generation capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKsQUrfNsDS5"
      },
      "source": [
        "## Retrieval Augmented Generation\n",
        "Traditional language models generate responses solely based on pre-learned patterns and information acquired during training. However, these models are inherently limited by the data on which they were trained, often leading to responses lacking depth or specific knowledge. RAG addresses this limitation by integrating external data when needed during the generation process. Here's how it works: When a query is made, the RAG system first retrieves relevant information from a large dataset or knowledge base. This information is then used to inform and guide the response generation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNChQCM-C5Q6"
      },
      "source": [
        "## Implementation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiZ6l2S1o-7Q"
      },
      "source": [
        "### PDF Ingestion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4juXRf8JNHT"
      },
      "outputs": [],
      "source": [
        "# Install the packages with support for all document types\n",
        "!pip install --q unstructured langchain\n",
        "!pip install --q \"unstructured[all-docs]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1jwHKnWJ36T"
      },
      "outputs": [],
      "source": [
        "# Import the modules from langchain_community package\n",
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "from langchain_community.document_loaders import OnlinePDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NubwkougK3Ea"
      },
      "outputs": [],
      "source": [
        "# Define the local path to the PDF file\n",
        "local_path = \"/path/to/file.pdf\" \n",
        "\n",
        "# Local PDF file uploads\n",
        "if local_path:\n",
        "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
        "  # Load the data from the PDF file\n",
        "  data = loader.load()\n",
        "else:\n",
        "  # Print a message if no PDF file is uploaded\n",
        "  print(\"Upload a PDF file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iTERAkwK6sB"
      },
      "outputs": [],
      "source": [
        "# Preview first page (optional)\n",
        "data[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A86vGB0ZPEsH"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyY27VGGN0xz"
      },
      "source": [
        "## Vector Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrp-I-NyOTkG"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "!pip install transformers\n",
        "!pip install --q chromadb\n",
        "!pip install --q langchain-text-splitters\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNMWeZjSLX1s"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoModel\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Replace the placeholder with your own HuggingFace API Token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"********\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luan6kb0RAUM"
      },
      "outputs": [],
      "source": [
        "# Split and chunk\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmQa1w4TXn2g"
      },
      "outputs": [],
      "source": [
        "# Generate Embeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ9DuKoIRLWs"
      },
      "outputs": [],
      "source": [
        "# Add to vector database\n",
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYs8gWQKYaSC"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gKDV_VHmYb8s"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "\n",
        "!pip3 install torch==2.0.1\n",
        "!pip3 install accelerate\n",
        "!pip3 install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR0F1-c5YRcd"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH93F4eHZh0F"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "                     model_kwargs={\n",
        "                          \"max_length\": 1000,  # Maximum length of the generated sequence\n",
        "                          \"max_new_tokens\": 10000,  # Maximum number of new tokens to generate\n",
        "                     })\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. \"\"\",\n",
        ")\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    db.as_retriever(),\n",
        "    llm,\n",
        "    prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "# RAG prompt\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJJwGVndvA0l"
      },
      "source": [
        "## Questioning the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltQVttQDahUd"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"What are the main trends identified in the report?\") # Place your question here\n",
        "print(\"Question:\",output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
